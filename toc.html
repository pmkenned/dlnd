<!DOCTYPE html>
<html>

<head>
</head>

<body>
	<ul>
		<li>Part 1: Introduction to Deep Learning	
			<ul>
				<li>Lesson 1: Welcome to Deep Learning
					<ol>
						<li><a href="p1_l1_s1.html" target="right">Welcome to the Deep Learning Nanodegree Program</a></li>
						<li><a href="p1_l1_s2.html" target="right">Meet Your Instructors</a></li>
						<li><a href="p1_l1_s3.html" target="right">Program Structure</a></li>
						<li><a href="p1_l1_s4.html" target="right">Projects You Will Build</a></li>
						<li><a href="p1_l1_s5.html" target="right">Deadline Policy</a></li>
						<li><a href="p1_l1_s6.html" target="right">Udacity Support</a></li>
						<li><a href="p1_l1_s7.html" target="right">Community Guidelines</a></li>
						<li><a href="p1_l1_s8.html" target="right">Prerequisites</a></li>
						<li><a href="p1_l1_s9.html" target="right">Getting Set Up</a></li>
					</ol>
				</li>
				<li>Lesson 2: Applying Deep Learning
					<ol>
						<li><a href="p1_l2_s1.html" target="right">Introduction</a></li>
						<li><a href="p1_l2_s2.html" target="right">Style Transfer</a></li>
						<li><a href="p1_l2_s3.html" target="right">DeepTraffic</a></li>
						<li><a href="p1_l2_s4.html" target="right">Flappy Bird</a></li>
						<li><a href="p1_l2_s5.html" target="right">Books to read</a></li>
					</ol>
				</li>
				<li>Lesson 3: Anaconda
					<ol>
						<li><a href="p1_l3_s1.html" target="right">Instructor</a></li>
						<li><a href="p1_l3_s2.html" target="right">Introduction</a></li>
						<li><a href="p1_l3_s3.html" target="right">What is Anaconda?</a></li>
						<li><a href="p1_l3_s4.html" target="right">Installing Anaconda</a></li>
						<li><a href="p1_l3_s5.html" target="right">Managing packages</a></li>
						<li><a href="p1_l3_s6.html" target="right">Managing environments</a></li>
						<li><a href="p1_l3_s7.html" target="right">More environmental actions</a></li>
						<li><a href="p1_l3_s8.html" target="right">Best practices</a></li>
						<li><a href="p1_l3_s9.html" target="right">On Python versions at Udacity</a></li>
					</ol>
				</li>
				<li>Lesson 4: Jupyter Notebooks
					<ol>
						<li><a href="p1_l4_s1.html" target="right">Instructor</a></li>
						<li><a href="p1_l4_s2.html" target="right">What are Jupyter notebooks?</a></li>
						<li><a href="p1_l4_s3.html" target="right">Installing Jupyter Notebook</a></li>
						<li><a href="p1_l4_s4.html" target="right">Launching the notebook server</a></li>
						<li><a href="p1_l4_s5.html" target="right">Notebook interface</a></li>
						<li><a href="p1_l4_s6.html" target="right">Code cells</a></li>
						<li><a href="p1_l4_s7.html" target="right">Markdown cells</a></li>
						<li><a href="p1_l4_s8.html" target="right">Keyboard shortcuts</a></li>
						<li><a href="p1_l4_s9.html" target="right">Magic keywords</a></li>
						<li><a href="p1_l4_s10.html" target="right">Converting notebooks</a></li>
						<li><a href="p1_l4_s11.html" target="right">Creating a slideshow</a></li>
						<li><a href="p1_l4_s12.html" target="right">Finishing up</a></li>
					</ol>
				</li>
				<li>Lesson 5: Matrix Math and NumPy Refresher
					<ol>
						<li>Introduction</li>
						<li>Data Dimensions</li>
						<li>Data in NumPy</li>
						<li>Element-wise Matrix Operations</li>
						<li>Element-wise Operations in NumPy</li>
						<li>Matrix Multiplication: Part 1</li>
						<li>Matrix Multiplication: Part 2</li>
						<li>NumPy Matrix Multiplication</li>
						<li>Matrix Transposes</li>
						<li>Tranposes in NumPy</li>
						<li>NumPy Quiz</li>
					</ol>
				</li>
			</ul>
		</li>
		<li>Part 2: Neural Networks
			<ul>
				<li>Lesson 1: Introduction to Neural Networks
					<ol>
						<li>Instructor</li>
						<li>Introduction</li>
						<li>Classification Problems 1</li>
						<li>Classification Problems 2</li>
						<li>Linear Boundaries</li>
						<li>Higher Dimensions</li>
						<li>Perceptrons</li>
						<li>Why "Neural Networks"?</li>
						<li>Perceptrons as Logical Operators</li>
						<li>Perceptron Trick</li>
						<li>Perceptron Algorithm</li>
						<li>Non-Linear Regions</li>
						<li>Error Functions</li>
						<li>Log-loss Error Function</li>
						<li>Discrete vs Continuous</li>
						<li>Softmax</li>
						<li>One-Hot Encoding</li>
						<li>Maximum Likelihood</li>
						<li>Maximizing Probabilities</li>
						<li>Cross-Entropy 1</li>
						<li>Cross-Entropy 2</li>
						<li>Multi-Class Cross Entropy</li>
						<li>Logistic Regression</li>
						<li>Gradient Descent</li>
						<li>Logistic Regression Algorithm</li>
						<li>Pre-Lab: Gradient Descent</li>
						<li>Lab: Gradient Descent</li>
						<li>Perceptron vs Gradient Descent</li>
						<li>Continuous Perceptrons</li>
						<li>Non-linear Data</li>
						<li>Non-linear Models</li>
						<li>Neural Network Architecture</li>
						<li>Feedforward</li>
						<li>Backpropagation</li>
						<li>Outro</li>
					</ol>
				</li>
				<li>Lesson 2: Implementing Gradient Descent
					<ol>
						<li>Mean Squared Error Function</li>
						<li>Gradient Descent</li>
						<li>Gradient Descent: The Math</li>
						<li>Gradient Descent: The Code</li>
						<li>Implementing Gradient Descent</li>
						<li>Multilayer Perceptrons</li>
						<li>Backpropagation</li>
						<li>Implementing Backpropagation</li>
						<li>Further Reading</li>
					</ol>
				</li>
				<li>Lesson 3: Training Neural Networks
					<ol>
						<li>Instructor</li>
						<li>Training Optimization</li>
						<li>Testing</li>
						<li>Overfitting and Underfitting</li>
						<li>Early Stopping</li>
						<li>Regularization</li>
						<li>Regularization 2</li>
						<li>Dropout</li>
						<li>Local Minima</li>
						<li>Random Restart</li>
						<li>Vanishing Gradient</li>
						<li>Other Activation Functions</li>
						<li>Batch vs Stochastic Gradient Descent</li>
						<li>Learning Rate Decay</li>
						<li>Momentum</li>
						<li>Error Functions Around the World</li>
					</ol>
				</li>
				<li>Lesson 4: GPU Workspaces Demo
				</li>
				<li>Project: Predicting Bike Sharing Data
				</li>
				<li>Lesson 6: Sentiment Analysis
					<ol>
						<li>Introducing Andrew Trask</li>
						<li>Meet Andrew</li>
						<li>Materials</li>
						<li>The Notebooks</li>
						<li>Framing the Problem</li>
						<li>Mini Project 1</li>
						<li>Mini Project 1 Solution</li>
						<li>Transforming Text into Numbers</li>
						<li>Mini Project 2</li>
						<li>Mini Project 2 Solutoin</li>
						<li>Building a Neural Network</li>
						<li>Mini Project 3</li>
						<li>Mini Project 3 Solution</li>
						<li>Understanding Neural Noise</li>
						<li>Mini Project 4</li>
						<li>Understanding inefficiencies in our Network</li>
						<li>Mini Project 5</li>
						<li>Mini Project 5 Solution</li>
						<li>Further Noise Reduction</li>
						<li>Mini Project 6</li>
						<li>Mini Project 6 Solution</li>
						<li>Analysis: What's Going on in the Weights?</li>
						<li>Conclusion</li>
					</ol>
				</li>
				<li>Lesson 7: Keras
					<ol>
						<li>Intro</li>
						<li>Keras</li>
						<li>Pre-Lab: Student Admissions in Keras</li>
						<li>Lab: Student Admissions in Keras</li>
						<li>Optimizers in Keras</li>
						<li>Mini Project Intro</li>
						<li>Pre-Lab: IMDB Data in Keras</li>
						<li>Lab: IMDB Data in Keras</li>
					</ol>
				</li>
				<li>Lesson 8: TensorFlow
					<ol>
						<li>Intro</li>
						<li>Installing TensorFlow</li>
						<li>Hello, Tensor World!</li>
						<li>Quiz: TensorFlow Input</li>
						<li>Quiz: TensorFlow Math</li>
						<li>Quiz: TensorFlow Linear Function</li>
						<li>Quiz: TensorFlow Softmax</li>
						<li>Quiz: TensorFlow Cross Entropy</li>
						<li>Quiz: Mini-batch</li>
						<li>Epochs</li>
						<li>Pre-Lab: NotMNIST in TensorFlow</li>
						<li>Lab: NotMNIST in TensorFlow</li>
						<li>Two-layer Neural Network</li>
						<li>Quiz: TensorFlow ReLUs</li>
						<li>Deep Neural Network in TensorFlow</li>
						<li>Save and Restore TensorFlow Models</li>
						<li>Finetuning</li>
						<li>Quiz: TensorFlow Dropout</li>
						<li>Outro</li>
					</ol>
				</li>
			</ul>
		</li>
		<li>Part 3: Convolutional Networks
			<ul>
				<li>Lesson 1: Cloud Computing
					<ol>
						<li>Overview</li>
						<li>Create an AWS Account</li>
						<li>Get Access to GPU Instances</li>
						<li>Apply Credits</li>
						<li>Launch an Instance</li>
						<li>Login to the Instance</li>
					</ol>
				</li>
				<li>Lesson 2: Convolutional Neural Networks</li>
					<ol>
						<li>Introducing Alexis</li>
						<li>Application of CNNs</li>
						<li>How Computers Interpret Images</li>
						<li>MLPs for Image Classification</li>
						<li>Categorical Cross-Entropy</li>
						<li>Model Validation in Keras</li>
						<li>When do MLPs (not) work well?</li>
						<li>Mini Project: Training an MLP on MNIST</li>
						<li>Local Connectivity</li>
						<li>Convolutional Layers (Part 1)</li>
						<li>Convolutional Layers (Part 2)</li>
						<li>Stride and Padding</li>
						<li>Convolutional Layers in Keras</li>
						<li>Quiz: Dimensionality</li>
						<li>Pooling Layers</li>
						<li>Max Pooling Layers in Keras</li>
						<li>CNNs for Image Classification</li>
						<li>CNNs in Keras: Practical Example</li>
						<li>Mini Project: CNNs in Keras</li>
						<li>Image Augmentation in Keras</li>
						<li>Mini Project: Image Augmentation in Keras</li>
						<li>Groundbreaking CNN Architectures</li>
						<li>Visualizing CNNs (Part 1)</li>
						<li>Visualizing CNNs (Part 2)</li>
						<li>Transfer Learning</li>
						<li>Transfer Learning in Keras</li>
					</ol>
				</li>
				<li>Lesson 3: CNNs in TensorFlow
					<ol>
						<li>Convolutional Layers</li>
						<li>Quiz: Convolutional Layers</li>
						<li>Solution: Convoluational Layers</li>
						<li>Max Pooling Layers</li>
						<li>Quiz: Max Pooling Layers</li>
						<li>Solution: Max Pooling Layers</li>
						<li>CNNs in TensorFlow</li>
						<li>CNNs - Additional Resources</li>
					</ol>
				</li>
				<li>Lesson 4: Weight Initialization
					<ol>
						<li>Weight Initialization Intro</li>
						<li>Ones and Zeros</li>
						<li>Uniform Distribution</li>
						<li>Too Small</li>
						<li>Normal Distribution</li>
						<li>Additional Material</li>
					</ol>
				</li>
				<li>Lesson 5: Autoencoders
					<ol>
						<li>Autoencoder Lesson Intro</li>
						<li>Autoencoders</li>
						<li>A Simple Autoencoder</li>
						<li>Simple Autoencoder Solution</li>
						<li>Convolutional Autoencoders</li>
						<li>Convolutional Autoencoders Solution</li>
					</ol>
				</li>
				<li>Lesson 6: Transfer Learning in TensorFlow
					<ol>
						<li>Transfer Learning Intro</li>
						<li>Transfer Learning with VGGNet</li>
						<li>VGGNet</li>
						<li>VGGNet Solution</li>
						<li>Data Preparation</li>
						<li>Data Preparation Solution</li>
						<li>Classifier</li>
						<li>Classifier Solution</li>
						<li>Training</li>
						<li>Training Solution</li>
					</ol>
				</li>
				<li>Project: Dog Breed Classifier</li>
				<li>Lesson 8: Deep Learning for Cancer Detection with Sebastian Thrun
					<ol>
						<li>Intro</li>
						<li>Skin Cancer</li>
						<li>Survival Probability of Skin Cancer</li>
						<li>Medical Classification</li>
						<li>The data</li>
						<li>Image Challenges</li>
						<li>Quiz: Data Challenges</li>
						<li>Solution: Data Challenges</li>
						<li>Training the Neural Network</li>
						<li>Quiz: Random vs Pre-initialized Weights</li>
						<li>Solution: Random vs Pre-initialized Weights</li>
						<li>Validating the Training</li>
						<li>Quiz: Sensitivity and Specificity</li>
						<li>Solution: Sensitivity and Specificity</li>
						<li>More on Sensitivity and Specificity</li>
						<li>Quiz: Diagnosing Cancer</li>
						<li>Solution: Diagnosing Cancer</li>
						<li>Refresh on ROC Curves</li>
						<li>Quiz: ROC Curve</li>
						<li>Solution: ROC Curve</li>
						<li>Comparing our Results with Doctors</li>
						<li>Visualization</li>
						<li>What is the network looking at?</li>
						<li>Refresh on Confusion Matrices</li>
						<li>Confusion Matrix</li>
						<li>Conclusion</li>
						<li>Useful Resources</li>
						<li>Mini Project Introduction</li>
						<li>Mini Project: Dermatologist AI</li>
						<li>Share Your Results!</li>
					</ol>
				</li>
			</ul>
		</li>
		<li>Part 4: Recurrent Networks
			<ul>
				<li>Lesson 1: Recurrent Neural Networks
					<ol>
						<li><a href="p4_l1_s1.html" target="right">Introducing Ortal</a></li>
						<li><a href="p4_l1_s2.html" target="right">RNN Introduction</a></li>
						<li><a href="p4_l1_s3.html" target="right">RNN History</a></li>
						<li><a href="p4_l1_s4.html" target="right">RNN Applications</a></li>
						<li><a href="p4_l1_s5.html" target="right">Feedforward Neural Network-Reminder</a></li>
						<li><a href="p4_l1_s6.html" target="right">The Feedforward Process</a></li>
						<li><a href="p4_l1_s7.html" target="right">Feedforward Quiz</a></li>
						<li><a href="p4_l1_s8.html" target="right">Backpropagation - Theory</a></li>
						<li><a href="p4_l1_s9.html" target="right">Backpropagation - Example (part a)</a></li>
						<li><a href="p4_l1_s10.html" target="right">Backpropagation - Example (part b)</a></li>
						<li><a href="p4_l1_s11.html" target="right">Backpropagation Quiz</a></li>
						<li><a href="p4_l1_s12.html" target="right">RNN (part a)</a></li>
						<li><a href="p4_l1_s13.html" target="right">RNN (part b)</a></li>
						<li><a href="p4_l1_s14.html" target="right">RNN - Unfolded Model</a></li>
						<li><a href="p4_l1_s15.html" target="right">Unfolded Model Quiz</a></li>
						<li><a href="p4_l1_s16.html" target="right">RNN - Example</a></li>
						<li><a href="p4_l1_s17.html" target="right">Backpropagation Through Time (part a)</a></li>
						<li><a href="p4_l1_s18.html" target="right">Backpropagation Through Time (part b)</a></li>
						<li><a href="p4_l1_s19.html" target="right">Backpropagation Through Time (part c)</a></li>
						<li><a href="p4_l1_s20.html" target="right">BPTT Quiz 1</a></li>
						<li><a href="p4_l1_s21.html" target="right">BPTT Quiz 2</a></li>
						<li><a href="p4_l1_s22.html" target="right">BPTT Quiz 3</a></li>
						<li><a href="p4_l1_s23.html" target="right">Some more math</a></li>
						<li><a href="p4_l1_s24.html" target="right">RNN Summary</a></li>
						<li><a href="p4_l1_s25.html" target="right">From RNN to LSTM</a></li>
						<li><a href="p4_l1_s26.html" target="right">Wrap Up</a></li>
					</ol>
				</li>
				<li>Lesson 2: Long Short-Term Memory Networks (LSTM)
					<ol>
						<li><a href="p4_l2_s1.html" target="right">Intro to LSTM</a></li>
						<li><a href="p4_l2_s2.html" target="right">RNN vs LSTM</a></li>
						<li><a href="p4_l2_s3.html" target="right">Basics of LSTM</a></li>
						<li><a href="p4_l2_s4.html" target="right">Architecture of LSTM</a></li>
						<li><a href="p4_l2_s5.html" target="right">The Learn Gate</a></li>
						<li><a href="p4_l2_s6.html" target="right">The Forget Gate</a></li>
						<li><a href="p4_l2_s7.html" target="right">The Remember Gate</a></li>
						<li><a href="p4_l2_s8.html" target="right">The Use Gate</a></li>
						<li><a href="p4_l2_s9.html" target="right">Putting it All Together</a></li>
						<li><a href="p4_l2_s10.html" target="right">Quiz</a></li>
						<li><a href="p4_l2_s11.html" target="right">Other architectures</a></li>
						<li><a href="p4_l2_s12.html" target="right">Outro LSTM</a></li>
					</ol>
				</li>
				<li>Lesson 3: Implementations of RNNs and LSTM
					<ol>
						<li><a href="p4_l3_s1.html" target="right">Intro</a></li>
						<li><a href="p4_l3_s2.html" target="right">Character-wise RNNs</a></li>
						<li><a href="p4_l3_s3.html" target="right">Sequence Batching</a></li>
						<li><a href="p4_l3_s4.html" target="right">Character-wise RNN Notebook</a></li>
						<li><a href="p4_l3_s5.html" target="right">Implementing a Character-wise RNN</a></li>
						<li><a href="p4_l3_s6.html" target="right">Batching Data Solution</a></li>
						<li><a href="p4_l3_s7.html" target="right">LSTM Cell</a></li>
						<li><a href="p4_l3_s8.html" target="right">LSTM Cell Solution</a></li>
						<li><a href="p4_l3_s9.html" target="right">RNN Output</a></li>
						<li><a href="p4_l3_s10.html" target="right">Network Loss</a></li>
						<li><a href="p4_l3_s11.html" target="right">Output and Loss Solutions</a></li>
						<li><a href="p4_l3_s12.html" target="right">Build the Network</a></li>
						<li><a href="p4_l3_s13.html" target="right">Build the Network Solution</a></li>
					</ol>
				</li>
				<li>Lesson 4: Hyperparameters
					<ol>
						<li>Introducing Jay</li>
						<li><a href="p4_l4_s2.html" target="right">Introduction</a></li>
						<li><a href="p4_l4_s3.html" target="right">Learning Rate</a></li>
						<li>Quiz: Learning Rate</li>
						<li>Minibatch Size</li>
						<li>Number of Training Iterations / Epochs</li>
						<li>Number of Hidden Units / Layers</li>
						<li>RNN Hyperparameters</li>
						<li>Quiz: RNN Hyperparameters</li>
						<li>Sources & References</li>
					</ol>
				</li>
				<li>Lesson 5: Embeddings and Word2vec
					<ol>
						<li>Embeddings Intro</li>
						<li>Implementing Word2Vec</li>
						<li>Subsampling Solution</li>
						<li>Making Batches</li>
						<li>Batches Solution</li>
						<li>Building the Network</li>
						<li>Negative Sampling</li>
						<li>Building the Network Solution</li>
						<li>Training Results</li>
					</ol>
				</li>
				<li>Lesson 6: Sentiment Prediction
					<ol>
						<li>Intro</li>
						<li>Sentiment RNN</li>
						<li>Data Preprocessing</li>
						<li>Creating Testing Sets</li>
						<li>Building the RNN</li>
						<li>Training the Network</li>
						<li>Solutions</li>
					</ol>
				</li>
				<li>Project: Generate TV Scripts</li>
			</ul>
		</li>
		<li>Part 5: Generative Adversarial Networks</li>
		<li>Part 6: Deep Reinforcement Learning</li>
	</ul>
</body>

</html>